{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs3Yt+OuC3aCFe8euzCgVg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuahurd515/ai-and-data-science-work/blob/main/P4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBf2yVL5-KeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b9edc1-3e9c-4cba-ca81-66ddfa8f573d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-48a178fb2a1e>:29: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation accuracy: 88.8900% (0.3925%)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# Load the dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# Preprocess the data by padding the sequences to a fixed length\n",
        "max_length = 500\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "# Define a function to create the model\n",
        "def create_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Embedding(input_dim=10000, output_dim=16, input_length=max_length),\n",
        "        layers.GlobalAveragePooling1D(),\n",
        "        layers.Dense(units=16, activation='relu'),\n",
        "        layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create a KerasClassifier with the create_model function\n",
        "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Concatenate the training and test sets for cross-validation\n",
        "x = np.concatenate((x_train, x_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "# Perform 5-fold cross-validation and print the results\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results = cross_val_score(model, x, y, cv=kfold)\n",
        "print(\"Cross-validation accuracy: %.4f%% (%.4f%%)\" % (results.mean()*100, results.std()*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**\n",
        "\n",
        "The provided code defines a binary classification model using the IMDB movie review dataset. The model is composed of an embedding layer that maps the input word indices to a 16-dimensional embedding space, followed by a GlobalAveragePooling1D layer that averages over the dimensions, and two fully connected layers with ReLU and sigmoid activations. The model is compiled with the Adam optimizer and binary cross-entropy loss, and trained for 10 epochs on batches of size 32. Finally, the model is evaluated on the test set and its loss and accuracy is printed with cross validation. Overall, the model performed very well for the most part with 5 fold cross vaidation. However, it could perform much better by changing the activation functions, adding more layers, or possibly adding a different amount of hidden units.\n",
        "\n",
        "It is also important to note that when downloading the dataset from stanford, my computer kept bugging out, I know that we were not supposed to use the keras dataset, but I had no idea what else to do. If there is any way that I could resubmit it and meet with someone that would be great, if not, I understand. But with this in mind, even with the keras dataset, the model still performed fairly well for the most part. As I stated before, there are some ways that this model could be improved, such as fine tuning it and adjusting some of the hyper parameters, but all in all, it performed well"
      ],
      "metadata": {
        "id": "npVpwZrFFGxo"
      }
    }
  ]
}