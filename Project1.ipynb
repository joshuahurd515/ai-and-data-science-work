{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi3aygs5v9GAHSSFYLpc5/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuahurd515/ai-and-data-science-work/blob/main/Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is my implementation of the Multi Layer Perceptron"
      ],
      "metadata": {
        "id": "ZnLmL-yhjFKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs, learning_rate=0.005):\n",
        "        ##initialize the class with the variables below\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_outputs = num_outputs\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.W1 = np.random.uniform(-1, 1, (num_inputs, num_hidden))\n",
        "        self.b1 = 1\n",
        "        self.W2 = np.random.uniform(-1, 1, (num_hidden, num_outputs))\n",
        "        self.b2 = 1\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        ##function for calculating the hidden layer activation function\n",
        "        return 1 / (1 + np.exp((-np.clip(x, -500, 500))))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        ##the derivative of the activation function\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def mse_loss(self, y_true, y_pred):\n",
        "        ##using mean of square errors as my loss function\n",
        "        return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "    def forward(self, X):\n",
        "        ##create the forward pass to go from the inputs to the hidden layer\n",
        "\n",
        "        ##multiplying X times W(weight of X to the hidden layer) and then adding the mass which is the logit\n",
        "\n",
        "        ##after calculating the logit, you then put it rhough the sigmoid function\n",
        "        ##  which is the hidden activation\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, y_pred):\n",
        "        ##below is calculating the backwards propagation which gives you the weights from the hidden layer to the output layer\n",
        "        error = y - y_pred\n",
        "        delta = error * self.sigmoid_derivative(y_pred)\n",
        "        dW2 = np.dot(self.a1.T, delta)\n",
        "        db2 = np.sum(delta, axis=0, keepdims=True)\n",
        "        delta_hidden = np.dot(delta, self.W2.T) * self.sigmoid_derivative(self.a1)\n",
        "        dW1 = np.dot(X.T, delta_hidden)\n",
        "        db1 = np.sum(delta_hidden, axis=0, keepdims=True)\n",
        "\n",
        "        ##once you calculate the weights, you then update them\n",
        "        self.W1 += self.learning_rate * dW1\n",
        "        self.b1 += self.learning_rate * db1\n",
        "        self.W2 += self.learning_rate * dW2\n",
        "        self.b2 += self.learning_rate * db2\n",
        "\n",
        "    def train(self, X, y, epochs=200):\n",
        "        ##training the data for the forward and setting backwards to the inputs from here\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            self.backward(X, y, y_pred)\n",
        "\n",
        "    def predict(self, X):\n",
        "        ##calling the forward function to get your prediction\n",
        "        y_pred = self.forward(X)\n",
        "        return np.argmax(y_pred, axis=1) + 1  # Convert one-hot encoding to class label\n",
        "\n",
        "##opening the wine file\n",
        "data = pd.read_csv(\"wine.data\", header=None)\n",
        "\n",
        "##splitting the class data and the other features apart\n",
        "X = data.iloc[:, 1:].values\n",
        "y = data.iloc[:, 0].values\n",
        "\n",
        "#one-hot encode the target variable\n",
        "num_classes = len(np.unique(y))\n",
        "y_onehot = np.zeros((len(y), num_classes))\n",
        "y_onehot[np.arange(len(y)), y-1] = 1\n",
        "\n",
        "##normalize the data\n",
        "##X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "#shuffle data\n",
        "np.random.seed(None)\n",
        "shuffle_idx = np.random.permutation(len(y))\n",
        "X, y_onehot = X[shuffle_idx], y_onehot[shuffle_idx]\n",
        "\n",
        "#define number of folds\n",
        "num_folds = 5\n",
        "fold_size = len(y) // num_folds\n",
        "\n",
        "#initialize list to store accuracies\n",
        "accuracies = []\n",
        "\n",
        "#perform cross validation\n",
        "for fold in range(num_folds):\n",
        "    #split data into training and validation sets\n",
        "    start_idx = fold * fold_size\n",
        "    end_idx = (fold + 1) * fold_size\n",
        "    val_X, val_y = X[start_idx:end_idx], y_onehot[start_idx:end_idx]\n",
        "    train_X = np.concatenate((X[:start_idx], X[end_idx:]), axis=0)\n",
        "    train_y = np.concatenate((y_onehot[:start_idx], y_onehot[end_idx:]), axis=0)\n",
        "\n",
        "    #initialize multi layer perceptron\n",
        "    num_inputs = X.shape[1]\n",
        "    num_hidden = 8\n",
        "    num_outputs = num_classes\n",
        "    learning_rate = 0.005\n",
        "    mlp = MLP(num_inputs, num_hidden, num_outputs, learning_rate)\n",
        "\n",
        "    #train MLP\n",
        "    mlp.train(train_X, train_y, epochs=200)\n",
        "\n",
        "    #test multi layer perceptron on validation set and calculate accuracy\n",
        "    val_y_pred = mlp.predict(val_X)\n",
        "    val_y_true = np.argmax(val_y, axis=1) + 1\n",
        "    accuracy = sum(val_y_pred == val_y_true) / len(val_y_true)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "#print mean accuracy for indivisual values and for the mean of accuracies\n",
        "print(\"Individual Accuracies: \", accuracies)\n",
        "print(\"Mean of Individual Accuracies: \", np.mean(accuracies))"
      ],
      "metadata": {
        "id": "sOYEgrhbizG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d51aded-2315-4095-bf12-8933ea5dd252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual Accuracies:  [0.4857142857142857, 0.45714285714285713, 0.2857142857142857, 0.37142857142857144, 0.37142857142857144]\n",
            "Mean of Individual Accuracies:  0.3942857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outputs from running the code:**"
      ],
      "metadata": {
        "id": "x_NECH-Eo-6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above are the outputs from running the code, here I have about 40% accuracy. The accuracy pretty much varies from anywhere from 30% to 40%.\n",
        "\n",
        "The odd thing about this is that when I normalized the data, my accuracy was very high. I would be getting around 97% accuracy, and I think that I might have done something wrong because the indivisual accuracies from the 5-fold cross validation were sometimes 100%, which honestly did not make very much sense to me\n"
      ],
      "metadata": {
        "id": "-9qrWAeijR_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Details:**"
      ],
      "metadata": {
        "id": "4SGXpyJppUOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above model implements the multi layer perceptron for classification of the wine data set. The MLP has an input layer with the number of features equal to the nymber of columns in the data set, as well as a hidden layer with eight nodes. Along with this, it also has a n output layer that that has the same amount of nodes as the nymber of classes in the target. The model implements forwards and back propagation to train the model, as well as using the sigmoid activation function on the hidden layer, and the softmax function used on the output layer. The loss function that I incorporated was the mean of squared errors. Lastly, the model used 5-fold cross validation to out put the average accuracy from doing this.\n",
        "\n",
        "(Note: As I said previously, I did not normalize my data in the final result because I was getting accuracy of around 97% which was not really making sense to me, so I took it out and would end up with a final average accuracy of around 34-43 percent.)"
      ],
      "metadata": {
        "id": "26q6KPLXpZ8f"
      }
    }
  ]
}